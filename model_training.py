# -*- coding: utf-8 -*-
"""v2-06-Abr-L1-L2-Optimization-Pos-Deep-Learning-Projeto-NLP-Sentinel-Projeto.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EM1U2WL0-wAQAfBnVyICqpG7qOdmVYD3

# üõ°Ô∏è NLP-Sentinel

## An√°lise de Sentimento em Avalia√ß√µes de Produtos

- #### Curso: P√≥s-Gradua√ß√£o em Deep Learning (CIn - UFPE)  
- ##### Disciplina: Projeto em Deep Learning  
  - ##### Professor: Cl√©ber Zanchettin  

- ##### Aluno:  
  - Luciano Ayres Farias de Carvalho (lafc@cin.ufpe.br)

---

## üìñ Sobre o Projeto

### üîç Vis√£o Geral  
O **NLP-Sentinel** √© um projeto de **Processamento de Linguagem Natural (PLN)** focado na an√°lise de sentimentos em avalia√ß√µes de produtos, utilizando **modelos de linguagem pr√©-treinados (LLMs)** e t√©cnicas modernas de **representa√ß√£o vetorial de senten√ßas** para gerar resultados eficientes sem depender de redes profundas treinadas do zero.

---

## üéØ Objetivos
- Classificar avalia√ß√µes de consumidores como **positivas**, **neutras** ou **negativas** com base no conte√∫do textual.
- **Correlacionar sentimentos** com as notas (de 1 a 5 estrelas) atribu√≠das pelos clientes.
- **Explorar padr√µes lingu√≠sticos** nas avalia√ß√µes, identificando t√≥picos recorrentes em diferentes polaridades.
- Comparar o desempenho de **modelos sem necessidade de fine-tuning** com abordagens mais tradicionais.

---

## ü§ñ Modelos e T√©cnicas Utilizadas

### üß† Representa√ß√£o Vetorial + Classifica√ß√£o:
- üîπ **SVM + Universal Sentence Encoder (USE)**  
  Abordagem leve e eficaz que combina embeddings sem√¢nticos do USE com um classificador SVM tradicional, gerando bons resultados com baixo custo computacional.

### üöÄ Modelos Pr√©-treinados:
- üîπ **BERT**, **DistilBERT**, **XLM-Roberta** (via Hugging Face Transformers)
- üîπ **Sentence-Transformers** para extra√ß√£o de embeddings de alta qualidade

### üß™ IA Generativa (LLMs):
- üîπ **In-Context Learning** com **OpenAI GPT**
- üîπ **In-Content Learning** com **Google Gemini**

> T√©cnicas como SVM com Bag of Words/Embeddings e redes neurais LSTM/RNN foram descontinuadas nesta vers√£o por apresentarem menor desempenho e alto custo de execu√ß√£o.

---

## üìä Base de Dados

O projeto utiliza o dataset [Olist Store](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce/data), um conjunto p√∫blico de e-commerce brasileiro contendo mais de 100 mil pedidos feitos entre 2016 e 2018.

- **Texto dos Coment√°rios**: avalia√ß√µes textuais feitas por consumidores
- **Review Score**: notas de 1 a 5 atribu√≠das aos produtos

> Os dados foram anonimizados e editados com nomes fict√≠cios (como casas de *Game of Thrones*) para garantir privacidade e permitir abordagens criativas.

---

## üß† Diferenciais do Projeto

- Foco em **simplicidade, desempenho e baixo custo computacional**
- Uso de **LLMs e embeddings sem√¢nticos** com alto poder de generaliza√ß√£o
- Integra√ß√£o com **IA generativa** sem necessidade de treinamento adicional
- Visualiza√ß√µes com **nuvens de palavras**, **matrizes de confus√£o** e **m√©tricas** claras

---

## üí° Por que o nome *NLP-Sentinel*?

Porque o sistema atua como um **sentinela inteligente**, sempre atento √†s emo√ß√µes, opini√µes e padr√µes revelados pelos consumidores ‚Äì extraindo insights diretamente da linguagem natural.

# Resumo dos Resultados

O projeto **NLP-Sentinel** investigou diferentes abordagens para classificar sentimentos (positivo ou negativo) em avalia√ß√µes reais de produtos da Olist, usando modelos de aprendizado supervisionado, embeddings sem√¢nticos e LLMs (Large Language Models).

---

#### ‚úÖ Desempenho dos Modelos Supervisionados

| Modelo                    | Acur√°cia (%) | F1-Score (%) |
|---------------------------|--------------|--------------|
| **BERT**                  | **94.26**    | **94.37**    |
| **RoBERTa**               | 93.83        | 93.94        |
| **DistilBERT**            | 93.23        | 93.37        |
| **SVM + Sentence-BERT**   | 92.88        | 93.01        |
| **SVM + USE**             | 92.59        | 92.72        |

---

#### ü§ñ Desempenho de LLMs (Zero-shot)

| Modelo               | Acur√°cia (%) | Acertos | Erros |
|----------------------|--------------|---------|-------|
| **OpenAI GPT-4**     | **100.0**    | 10      | 0     |
| **Google Gemini 1.5**| 90.0         | 9       | 1     |

> üîé **Destaque:** O modelo **BERT** fine-tunado teve o melhor desempenho entre os supervisionados. O **GPT-4** obteve **100% de acur√°cia** mesmo sem treinamento, mostrando a for√ßa dos modelos generativos modernos com *In-Context Learning*.

---

### ‚úÖ **Conclus√£o**

O projeto **NLP-Sentinel** demonstrou que √© poss√≠vel alcan√ßar alta precis√£o na an√°lise de sentimentos em portugu√™s por meio de diferentes estrat√©gias. A experimenta√ß√£o com modelos supervisionados, embeddings e LLMs proporcionou insights valiosos sobre desempenho, custo e aplicabilidade em cen√°rios reais.

---

#### üß† Principais Aprendizados

- üí° **Modelos h√≠bridos**, como **Sentence-BERT combinado com SVM**, mostraram-se bastante competitivos, entregando bons resultados com baixo consumo de recursos ‚Äî ideais para aplica√ß√µes com restri√ß√µes computacionais.

- üöÄ **Modelos generativos como GPT-4 e Gemini** demonstraram excelente desempenho **sem necessidade de treinamento adicional**, revelando-se altamente eficazes para tarefas pontuais e prot√≥tipos r√°pidos com *zero-shot learning*.

- ‚öñÔ∏è **Simplicidade e efici√™ncia importam**: T√©cnicas como **SVM com Bag of Words/Embeddings** e **redes neurais LSTM/RNN** foram **abandonadas** nesta vers√£o do projeto por apresentarem **baixo desempenho e alto custo de execu√ß√£o**, refor√ßando a import√¢ncia da an√°lise criteriosa de retorno t√©cnico e operacional.

- üßΩ **Pr√©-processamento e curadoria de dados** foram essenciais para o sucesso do projeto. A remo√ß√£o de ru√≠do, normaliza√ß√£o de textos e balanceamento de classes influenciaram diretamente o desempenho dos modelos.

- üîÅ **Publicar modelos no Hugging Face Hub** simplificou o versionamento, o reuso e a colabora√ß√£o, al√©m de deixar o projeto mais organizado e acess√≠vel.

- üéØ A escolha do modelo ideal deve considerar n√£o apenas **m√©tricas de performance**, mas tamb√©m fatores como **custo computacional, facilidade de deploy, manuten√ß√£o e interpretabilidade**.

# Setup do Ambiente & Depend√™ncias

## Instala√ß√µes & Downloads

### Resumo

#### ‚öôÔ∏è Instala√ß√£o e Prepara√ß√£o do Ambiente

Antes de iniciar o processamento e a an√°lise dos dados, o ambiente √© configurado com as bibliotecas e recursos necess√°rios para execu√ß√£o dos modelos e pr√©-processamento de texto.

#### üì¶ Instala√ß√£o de Pacotes
O projeto depende de uma s√©rie de bibliotecas para Processamento de Linguagem Natural, aprendizado de m√°quina e integra√ß√£o com modelos de linguagem avan√ßados. Entre elas:

- `nltk`
- `spacy`
- `transformers`
- `huggingface_hub`
- `joblib`
- `wordcloud`
- `sentence-transformers`
- `tensorflow-text`
- `google-generativeai`
- `langchain-openai`

Tamb√©m √© realizado o comando `git lfs install` para suporte ao versionamento de arquivos grandes via Git LFS.

#### üßπ Recursos do NLTK
S√£o baixados recursos lingu√≠sticos essenciais para o pr√©-processamento de texto, como:

- Stopwords em portugu√™s
- Tokenizadores (`punkt` e `punkt_tab`)

#### üåç Modelo Lingu√≠stico do spaCy
√â feito o download do modelo de l√≠ngua portuguesa `pt_core_news_md`, utilizado para an√°lise lexical, lematiza√ß√£o e outras tarefas de PLN com alta precis√£o.

### C√≥digo
"""

# Install required packages (suppressing output for brevity)
!pip install -q nltk spacy transformers huggingface_hub joblib wordcloud sentence-transformers tensorflow-text google-generativeai langchain-openai
!git lfs install

# Download NLTK stopwords (and any other required NLTK data)
import nltk
nltk.download("stopwords")
nltk.download("punkt")
nltk.download("punkt_tab")

# Download spaCy's Portuguese model
!python -m spacy download pt_core_news_md

"""## Imports & Configura√ß√µes

### Resumo

#### üìö Bibliotecas e Configura√ß√µes Utilizadas

O projeto **NLP-Sentinel** faz uso de um ecossistema robusto de bibliotecas voltadas √† an√°lise de dados, Processamento de Linguagem Natural (PLN), aprendizado de m√°quina e uso de modelos de linguagem avan√ßados (LLMs). A seguir, as categorias principais:

#### üì¶ Manipula√ß√£o de Dados e Visualiza√ß√£o
Utiliza bibliotecas como **Pandas** e **NumPy** para organiza√ß√£o e an√°lise dos dados. Ferramentas como **Matplotlib**, **Seaborn** e **WordCloud** s√£o usadas para criar visualiza√ß√µes e nuvens de palavras que facilitam a compreens√£o dos padr√µes de linguagem nas avalia√ß√µes.

#### üßπ Processamento de Linguagem Natural (PLN)
O pr√©-processamento textual √© feito com **NLTK** e **spaCy**, realizando tokeniza√ß√£o, remo√ß√£o de stopwords, lematiza√ß√£o e an√°lise l√©xica. Tamb√©m s√£o utilizadas bibliotecas auxiliares para manipula√ß√£o de textos e frequ√™ncias de palavras.

#### üî† Embeddings e Modelos Pr√©-treinados
O projeto integra **Universal Sentence Encoder (USE)** por meio do TensorFlow Hub e modelos do **Sentence-Transformers** para gerar representa√ß√µes vetoriais sem√¢nticas. Al√©m disso, emprega modelos pr√©-treinados como **BERT**, **DistilBERT** e **XLM-Roberta** utilizando a biblioteca **Transformers** da Hugging Face.

#### üß† Aprendizado de M√°quina
Utiliza algoritmos e utilit√°rios da **Scikit-learn** para tarefas de vetoriza√ß√£o, classifica√ß√£o com SVM, modelagem de t√≥picos, avalia√ß√£o de desempenho e pipelines de modelagem. T√©cnicas de otimiza√ß√£o como busca em grade (Grid Search) tamb√©m s√£o aplicadas para aprimorar os modelos.

#### ‚öôÔ∏è Modelagem com TensorFlow e Keras
Alguns testes explorat√≥rios foram realizados com redes neurais utilizando **TensorFlow** e **Keras**, empregando camadas como LSTM, Dense e Dropout. A configura√ß√£o inclui regulariza√ß√µes e parametriza√ß√µes b√°sicas para controle de treinamento.

#### ü§ñ IA Generativa e In-Context Learning
A arquitetura moderna do projeto inclui o uso de **modelos generativos** em experimentos de *In-Context Learning*, com integra√ß√£o √† **OpenAI (via LangChain)** e √† **plataforma Gemini da Google**.

#### üîê Autentica√ß√£o e Configura√ß√µes
As **chaves de API** da Hugging Face, OpenAI e Google s√£o acessadas com seguran√ßa atrav√©s de vari√°veis armazenadas no ambiente do Google Colab. O projeto tamb√©m define par√¢metros globais para padronizar o n√∫mero de √©pocas e o tamanho dos lotes durante os testes de modelos.

### C√≥digo
"""

# Import core libraries for data handling, visualization, and machine learning
import os
import shutil
import subprocess
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import string
import tensorflow as tf

# NLP and text processing libraries
from nltk.corpus import stopwords
from nltk import word_tokenize
from wordcloud import WordCloud
import spacy
from collections import Counter

# Transformers and Hugging Face related libraries
from transformers import BertTokenizer, TFBertForSequenceClassification, AdamWeightDecay, DistilBertTokenizer, TFDistilBertForSequenceClassification, XLMRobertaTokenizer, TFXLMRobertaForSequenceClassification
import joblib
from huggingface_hub import create_repo, Repository
from sentence_transformers import SentenceTransformer

# Additional libraries for In-Context Learning experiments
import google.generativeai as genai
from langchain_openai import ChatOpenAI

# TensorFlow Text (required for Universal Sentence Encoder, etc.)
import tensorflow_text as text

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Layer, Bidirectional, Embedding, SimpleRNN, Dense, Dropout, Input
from tensorflow.keras import regularizers
import tensorflow_hub as hub

from tensorflow.keras.layers import LSTM

# Sklearn
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.model_selection import train_test_split

from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, ConfusionMatrixDisplay

from sklearn.preprocessing import StandardScaler

# Google Colab
from google.colab import userdata

# Get Hugging Face token from Secrets
hf_token = userdata.get('HUGGING_FACE_TOKEN')

# Get OpenAI API Key from Secrets
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')

# Retorna a chave de API do Google Gemini do userdata
myKey = userdata.get('GOOGLE_API_KEY')

# Set global configuration parameters
DEFAULT_EPOCHS = 1
DEFAULT_BATCH_SIZE = 32

print("Environment setup complete. All dependencies have been installed and imported.")

"""# Carregamento dos Dados & Explora√ß√£o Inicial

### Resumo

#### üßæ Carregamento e Inspe√ß√£o dos Dados

O projeto inicia com o carregamento e a inspe√ß√£o explorat√≥ria do conjunto de dados, garantindo que os dados estejam √≠ntegros e prontos para as etapas seguintes de pr√©-processamento e modelagem.

#### üì• Fonte dos Dados
O dataset utilizado est√° dispon√≠vel publicamente no GitHub e √© carregado diretamente via URL no formato `.csv`. Ele cont√©m avalia√ß√µes textuais e notas atribu√≠das por clientes de e-commerce no Brasil.

#### üîç Carregamento do Dataset
Uma fun√ß√£o √© utilizada para ler o arquivo CSV e armazenar os dados em um `DataFrame` do Pandas. Ao final do processo, uma mensagem de confirma√ß√£o √© exibida indicando que os dados foram carregados com sucesso.

#### üìä Inspe√ß√£o Inicial dos Dados
Uma segunda fun√ß√£o realiza uma inspe√ß√£o b√°sica do `DataFrame`, exibindo:
- Informa√ß√µes gerais sobre colunas e tipos de dados
- Estat√≠sticas descritivas (inclusive para vari√°veis categ√≥ricas)
- As 5 primeiras linhas do conjunto de dados

Essa etapa √© essencial para entender a estrutura dos dados e orientar decis√µes sobre tratamento posterior.

#### üö® Verifica√ß√£o de Dados Ausentes e Inconsistentes
Tamb√©m √© feita uma an√°lise para detectar:
- **Valores nulos (NaN)** por coluna
- **Campos vazios** representados por strings em branco

Esses diagn√≥sticos ajudam a identificar poss√≠veis problemas de qualidade nos dados e guiam o pr√©-processamento necess√°rio antes da vetoriza√ß√£o e classifica√ß√£o.

### C√≥digo
"""

def load_data(url: str) -> pd.DataFrame:
    """
    Loads a CSV dataset from the provided URL.

    Args:
        url (str): URL of the CSV file.

    Returns:
        pd.DataFrame: Loaded dataset.
    """
    df = pd.read_csv(url)
    print("Dataset loaded successfully!")
    return df

def inspect_data(df: pd.DataFrame):
    """
    Displays basic information about the DataFrame including data types, summary statistics,
    and the first few rows.

    Args:
        df (pd.DataFrame): DataFrame to inspect.
    """
    print("\n--- DataFrame Info ---")
    df.info()

    print("\n--- Descriptive Statistics (All Columns) ---")
    # For comprehensive statistics including categorical variables
    display(df.describe(include='all'))

    print("\n--- First 5 Rows of the DataFrame ---")
    display(df.head())

def check_missing_and_inconsistencies(df: pd.DataFrame):
    """
    Checks and prints the count of missing values and empty strings in the DataFrame.

    Args:
        df (pd.DataFrame): DataFrame to check.
    """
    print("\n--- Missing Values by Column ---")
    nan_counts = df.isnull().sum()
    nan_counts = nan_counts[nan_counts > 0]
    print(nan_counts)

    print("\n--- Empty String Counts by Column ---")
    empty_counts = df.apply(lambda col: (col.astype(str).str.strip() == "").sum())
    empty_counts = empty_counts[empty_counts > 0]
    print(empty_counts)

# URL of the dataset
data_url = "https://raw.githubusercontent.com/lucianoayres/filebin/refs/heads/main/olist_order_reviews_dataset.csv"

# Load the dataset
df_new = load_data(data_url)

# Perform basic inspection of the data
inspect_data(df_new)

# Check for missing values and inconsistent data (empty strings)
check_missing_and_inconsistencies(df_new)

"""# Limpeza dos Dados & Pr√©-processamento

### Resumo

#### üßº Limpeza, Transforma√ß√£o e Mapeamento de Sentimentos

Ap√≥s o carregamento inicial, os dados passam por diversas etapas de padroniza√ß√£o, filtragem e transforma√ß√£o para garantir consist√™ncia e qualidade antes da vetoriza√ß√£o e classifica√ß√£o.

#### üìä Estat√≠sticas em Cada Etapa
Em diferentes pontos do processo, s√£o exibidas estat√≠sticas b√°sicas do dataset:
- Quantidade total de linhas
- Valores ausentes por coluna
- Distribui√ß√£o da coluna `rating` (quando dispon√≠vel)

Essas estat√≠sticas s√£o √∫teis para acompanhar a evolu√ß√£o dos dados ao longo das etapas de tratamento.

#### üè∑Ô∏è Renomea√ß√£o e Combina√ß√£o de Colunas
As colunas do dataset s√£o renomeadas para nomes padronizados. Al√©m disso:
- Os campos `review_comment_title` e `review_comment_message` s√£o combinados em uma √∫nica coluna `review`.
- Essa fus√£o garante que tanto o t√≠tulo quanto o coment√°rio sejam considerados na an√°lise de sentimento.

#### üßπ Filtragem de Dados Inv√°lidos
V√°rias regras s√£o aplicadas para eliminar registros irrelevantes ou inconsistentes:
- Remo√ß√£o de avalia√ß√µes com nota 3 (consideradas neutras ou amb√≠guas)
- Convers√£o e valida√ß√£o de ratings como valores num√©ricos
- Elimina√ß√£o de coment√°rios vazios ou compostos apenas por espa√ßos
- Restri√ß√£o a ratings v√°lidos: **1, 2, 4, 5**

Essas a√ß√µes visam deixar o conjunto mais limpo e orientado √† classifica√ß√£o bin√°ria.

#### ‚ù§Ô∏è Mapeamento de Ratings para Sentimentos
Os ratings s√£o transformados em duas novas colunas:
- **`sentiment`**: etiqueta textual (`positivo` ou `negativo`)
- **`polarity`**: valor bin√°rio (1 para positivo, 0 para negativo)

Essa transforma√ß√£o facilita a aplica√ß√£o de algoritmos supervisionados e serve como base para avalia√ß√£o dos modelos.

#### üíæ Salvamento do Dataset Final
O conjunto de dados final, devidamente limpo e rotulado, √© exportado para um novo arquivo CSV. Esse arquivo servir√° como base para as pr√≥ximas etapas de vetoriza√ß√£o, modelagem e avalia√ß√£o de desempenho.

### C√≥digo
"""

def print_stats(df: pd.DataFrame, stage: str = ""):
    """
    Imprime estat√≠sticas b√°sicas do DataFrame, incluindo:
      - N√∫mero total de linhas
      - Quantidade de valores ausentes por coluna
      - Distribui√ß√£o da coluna 'rating', se existir

    Args:
        df (pd.DataFrame): DataFrame a ser analisado.
        stage (str): Descri√ß√£o do est√°gio (ex: "Dados Originais", "Ap√≥s Filtragem", etc.)
    """
    print(f"--- Estat√≠sticas {stage} ---")
    print(f"Total de linhas: {len(df)}")
    print("Valores ausentes por coluna:")
    print(df.isnull().sum())
    if "rating" in df.columns:
        print("Distribui√ß√£o de ratings:")
        print(df["rating"].value_counts())
    print("-" * 40)


def rename_and_combine_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    Renomeia as colunas para nomes padronizados e combina 'review_title' e 'review_comment_message'
    em uma √∫nica coluna 'review'.

    Args:
        df (pd.DataFrame): DataFrame original.

    Retorna:
        pd.DataFrame: DataFrame com colunas renomeadas e review combinada.
    """
    # Renomeia as colunas
    df = df.rename(columns={
        "review_comment_message": "review",
        "review_comment_title": "review_title",
        "review_score": "rating"
    })

    # Fun√ß√£o para combinar t√≠tulo e review
    def combine_review(row):
        title = str(row["review_title"]) if not pd.isnull(row["review_title"]) else ""
        review = str(row["review"]) if not pd.isnull(row["review"]) else ""
        if review.strip() != "":
            if title.strip() != "":
                return title.strip() + ". " + review.strip()
            else:
                return review.strip()
        else:
            return title.strip()

    df["review"] = df.apply(combine_review, axis=1)

    # Mant√©m apenas as colunas necess√°rias
    df = df[["review", "rating"]]
    return df


def filter_invalid_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    Filtra os dados removendo:
      - Linhas com rating igual a 3 (considerado neutro/outlier)
      - Ratings n√£o num√©ricos
      - Linhas com reviews vazias ou compostas apenas por espa√ßos
      - Ratings que n√£o est√£o no conjunto v√°lido [1, 2, 4, 5]

    Args:
        df (pd.DataFrame): DataFrame de entrada.

    Retorna:
        pd.DataFrame: DataFrame limpo ap√≥s a filtragem.
    """
    # Cria uma c√≥pia para evitar SettingWithCopyWarning
    df = df.copy()

    # Remove linhas com rating igual a 3
    df = df[df["rating"] != 3]

    # Garante que 'rating' seja num√©rico; converte erros para NaN e remove linhas com NaN
    df.loc[:, "rating"] = pd.to_numeric(df["rating"], errors='coerce')
    df = df.dropna(subset=["rating"])

    # Remove linhas com reviews vazias (ap√≥s remover espa√ßos)
    df = df[~df["review"].apply(lambda x: isinstance(x, str) and x.strip() == "")]

    # Filtra para manter apenas ratings v√°lidos (1, 2, 4, 5)
    valid_ratings = [1, 2, 4, 5]
    df = df[df["rating"].isin(valid_ratings)]

    return df


def map_ratings_to_labels(df: pd.DataFrame) -> pd.DataFrame:
    """
    Converte os ratings num√©ricos em labels de sentimento e cria uma coluna de polaridade bin√°ria.
    Ratings maiores ou iguais a 4 s√£o considerados 'positivo'; ratings menores ou iguais a 2 s√£o 'negativo'.

    Args:
        df (pd.DataFrame): DataFrame com a coluna 'rating'.

    Retorna:
        pd.DataFrame: DataFrame com as colunas 'sentiment' e 'polarity' adicionadas.
    """
    def convert_to_sentiment(rating):
        return "positivo" if rating >= 4 else "negativo"

    def convert_to_polarity(rating):
        return 1 if rating >= 4 else 0

    df["sentiment"] = df["rating"].apply(convert_to_sentiment)
    df["polarity"] = df["rating"].apply(convert_to_polarity)
    return df

print_stats(df_new, "Dados Originais")

"""#### 1. Renomear e combinar colunas

"""

df_clean = rename_and_combine_columns(df_new)
print_stats(df_clean, "Ap√≥s Renomear e Combinar Colunas")

"""#### 2. Filtrar dados inv√°lidos"""

df_clean = filter_invalid_data(df_clean)
print_stats(df_clean, "Ap√≥s Filtragem de Dados Inv√°lidos")

"""#### 3. Mapear ratings para labels e polaridade"""

df_clean = map_ratings_to_labels(df_clean)
print_stats(df_clean, "Ap√≥s Mapeamento de Ratings para Sentimentos")

"""#### 4. Salvar os dados limpos para refer√™ncia futura

"""

df_clean.to_csv("olist_order_reviews_dataset.csv", index=False)

"""#### 5. Exibir Amostra do Dataframe Final Ap√≥s a Limpeza"""

# Atualiza o DataFrame principal
df = df_clean

# Exibe as primeiras linhas do DataFrame final limpo
display(df.head())

"""# An√°lise Descritiva

### Resumo

#### üìä An√°lise Explorat√≥ria dos Dados (EDA)

Ap√≥s a prepara√ß√£o do dataset, √© realizada uma an√°lise explorat√≥ria detalhada para entender padr√µes, varia√ß√µes e correla√ß√µes nos dados de avalia√ß√£o dos consumidores.

---

#### üìà Distribui√ß√£o das Notas (Ratings)
Um gr√°fico de barras exibe a frequ√™ncia de cada nota (de 1 a 5), permitindo visualizar o desequil√≠brio de classes no conjunto de dados. Tamb√©m s√£o listados os totais de avalia√ß√µes por nota.

---

#### üìè An√°lise do Comprimento das Avalia√ß√µes
Foram calculadas duas m√©tricas para cada coment√°rio:
- **N√∫mero de caracteres**
- **N√∫mero de palavras**

Essas m√©tricas foram visualizadas em histogramas e boxplots, segmentados por sentimento (positivo/negativo), para observar diferen√ßas na extens√£o textual conforme a polaridade da avalia√ß√£o.

---

#### ‚òÅÔ∏è Nuvens de Palavras (Word Clouds)
Foram geradas nuvens de palavras separadas para avalia√ß√µes positivas e negativas. Essa visualiza√ß√£o qualitativa revela as palavras mais recorrentes em cada polaridade, ajudando a entender o vocabul√°rio t√≠pico dos consumidores satisfeitos e insatisfeitos.

---

#### üìâ Rela√ß√£o entre Comprimento e Nota
Um gr√°fico de dispers√£o com linha de regress√£o mostra a correla√ß√£o entre o n√∫mero de caracteres das avalia√ß√µes e a nota atribu√≠da. O coeficiente de correla√ß√£o de Pearson tamb√©m √© calculado para quantificar essa rela√ß√£o.

---

#### üßæ Extra√ß√£o de Entidades Nomeadas (NER)
Usando o modelo de linguagem do **spaCy**, foram extra√≠das entidades nomeadas como organiza√ß√µes, pessoas, locais e produtos. As 20 entidades mais mencionadas nas avalia√ß√µes s√£o exibidas em um gr√°fico de barras.

---

#### üß† Riqueza Lexical
Foram calculadas as seguintes m√©tricas para cada review:
- Total de palavras (`word_count`)
- Palavras √∫nicas (`unique_word_count`)
- Taxa de diversidade lexical (TTR ‚Äì *Type-Token Ratio*)

Estat√≠sticas descritivas dessas m√©tricas foram analisadas, al√©m da m√©dia de palavras por sentimento e um histograma comparativo.

---

#### üî° An√°lise de N-gramas (Bigrams e Trigrams)
A an√°lise de n-gramas permite identificar combina√ß√µes de palavras frequentes:
- **Top 10 bigramas** (ex: "entrega r√°pida", "produto bom")
- **Top 10 trigramas** (ex: "muito bom produto")

Essa t√©cnica ajuda a identificar frases recorrentes e express√µes comuns nas avalia√ß√µes.

---

#### üßµ Modelagem de T√≥picos (LDA)
Com o uso de **Latent Dirichlet Allocation (LDA)**, foram identificados **cinco t√≥picos** principais dentro das avalia√ß√µes, destacando palavras-chave associadas a cada grupo tem√°tico.

---

#### üîó Matriz de Correla√ß√£o
Foi constru√≠da uma matriz de correla√ß√£o entre vari√°veis num√©ricas como:
- Nota (rating)
- Polaridade (bin√°ria)
- Total de palavras
- TTR

Essa matriz foi visualizada em formato de **heatmap** para facilitar a interpreta√ß√£o visual das rela√ß√µes entre vari√°veis.

---

Essa an√°lise explorat√≥ria fornece uma base s√≥lida para interpretar o comportamento dos consumidores e embasar as decis√µes de modelagem no projeto.

### C√≥digo

#### 1. Distribui√ß√£o das Notas
"""

plt.figure(figsize=(8, 6))
ax = sns.countplot(x='rating', data=df, hue='rating', palette='pastel')
if ax.get_legend() is not None:
    ax.get_legend().remove()  # Remove legend to avoid duplication
plt.title('Distribui√ß√£o das Notas')
plt.xlabel('Nota')
plt.ylabel('Frequ√™ncia')
plt.show()

rating_counts = df['rating'].value_counts().sort_index()
print("Resultados Totais da Distribui√ß√£o das Notas:")
for rating, count in rating_counts.items():
    print(f"Nota {rating}: {count} avalia√ß√µes")

"""#### 2. An√°lise do Comprimento das Avalia√ß√µes (Caracteres e Palavras)

"""

df['review_length_chars'] = df['review'].str.len()
df['review_length_words'] = df['review'].str.split().apply(len)

plt.figure(figsize=(8, 6))
plt.hist(df['review_length_chars'], bins=50, color='skyblue', edgecolor='black')
plt.title('Histograma do Comprimento das Avalia√ß√µes (Caracteres)')
plt.xlabel('N√∫mero de caracteres')
plt.ylabel('Frequ√™ncia')
plt.show()

plt.figure(figsize=(8, 6))
plt.hist(df['review_length_words'], bins=50, color='lightcoral', edgecolor='black')
plt.title('Histograma do Comprimento das Avalia√ß√µes (Palavras)')
plt.xlabel('N√∫mero de palavras')
plt.ylabel('Frequ√™ncia')
plt.show()

"""#### 3. Box Plot do Comprimento das Avalia√ß√µes por Sentimento

"""

plt.figure(figsize=(8, 6))
ax = sns.boxplot(x='sentiment', y='review_length_chars', data=df, hue='sentiment', palette='pastel')
if ax.get_legend() is not None:
    ax.get_legend().remove()  # Remove the legend if it exists
plt.title('Distribui√ß√£o do Comprimento das Avalia√ß√µes por Sentimento (Caracteres)')
plt.xlabel('Sentimento')
plt.ylabel('N√∫mero de caracteres')
plt.show()

plt.figure(figsize=(8, 6))
ax = sns.boxplot(x='sentiment', y='review_length_words', data=df, hue='sentiment', palette='pastel')
if ax.get_legend() is not None:
    ax.get_legend().remove()  # Remove the legend if it exists
plt.title('Distribui√ß√£o do Comprimento das Avalia√ß√µes por Sentimento (Palavras)')
plt.xlabel('Sentimento')
plt.ylabel('N√∫mero de palavras')
plt.show()

"""#### 4. Nuvens de Palavras para Avalia√ß√µes Positivas e Negativas"""

def generate_wordcloud(text, title):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(title)
    plt.show()

# Use a coluna 'clean_review' se estiver dispon√≠vel; caso contr√°rio, utilize 'review'
if 'clean_review' in df.columns:
    pos_text = ' '.join(df[df['sentiment'] == 'positivo']['clean_review'])
    neg_text = ' '.join(df[df['sentiment'] == 'negativo']['clean_review'])
else:
    pos_text = ' '.join(df[df['sentiment'] == 'positivo']['review'])
    neg_text = ' '.join(df[df['sentiment'] == 'negativo']['review'])

generate_wordcloud(pos_text, "Nuvem de Palavras - Avalia√ß√µes Positivas")

generate_wordcloud(neg_text, "Nuvem de Palavras - Avalia√ß√µes Negativas")

"""#### 5. An√°lise de Correla√ß√£o: Comprimento da Avalia√ß√£o vs. Nota

"""

plt.figure(figsize=(8, 6))
sns.regplot(x='review_length_chars', y='rating', data=df, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})
plt.title('Rela√ß√£o entre Comprimento da Avalia√ß√£o e Nota')
plt.xlabel('N√∫mero de caracteres')
plt.ylabel('Nota')
plt.show()

correlation = df['review_length_chars'].corr(df['rating'])
print(f"Coeficiente de correla√ß√£o de Pearson entre comprimento e nota: {correlation:.2f}")

"""#### 6. An√°lise de Entidades Nomeadas (NER)"""

nlp = spacy.load('pt_core_news_md')

def extract_entities(text):
    doc = nlp(text)
    # Seleciona entidades de interesse (ex.: ORGANIZA√á√ÉO, PESSOA, LOCAL, PRODUTO)
    return [ent.text for ent in doc.ents if ent.label_ in ['ORG', 'PERSON', 'LOC', 'PRODUCT']]

df['entities'] = df['review'].astype(str).apply(extract_entities)
all_entities = [entity for sublist in df['entities'] for entity in sublist]
entity_freq = Counter(all_entities).most_common(20)
entities_df = pd.DataFrame(entity_freq, columns=['Entidade', 'Frequ√™ncia'])

plt.figure(figsize=(12, 8))
ax = sns.barplot(x='Frequ√™ncia', y='Entidade', data=entities_df, hue='Entidade', palette='viridis')
if ax.get_legend() is not None:
    ax.get_legend().remove()  # Remove the legend if it exists
plt.title('Top 20 Entidades Nomeadas Mais Frequentes')
plt.xlabel('Frequ√™ncia')
plt.ylabel('Entidade')
plt.tight_layout()
plt.show()

"""#### 7. Riqueza L√©xica & Legibilidade"""

def compute_lexical_metrics(text: str):
    # Basic tokenization
    tokens = nltk.word_tokenize(text.lower())
    num_words = len(tokens)
    unique_words = set(tokens)
    num_unique = len(unique_words)
    ttr = num_unique / num_words if num_words > 0 else 0
    return num_words, num_unique, ttr

df["word_count"] = 0
df["unique_word_count"] = 0
df["ttr"] = 0.0

for i, row in df.iterrows():
    wc, uwc, ttr_value = compute_lexical_metrics(row["review"])
    df.at[i, "word_count"] = wc
    df.at[i, "unique_word_count"] = uwc
    df.at[i, "ttr"] = ttr_value

# Summarize these new columns
print("\n--- Lexical Richness Summary ---")
print(df[["word_count", "unique_word_count", "ttr"]].describe())

# Compare average word count by sentiment
avg_word_count = df.groupby("sentiment")["word_count"].mean()
print("\n--- Average Word Count by Sentiment ---")
print(avg_word_count)

# Optional: Visualize word count distribution
plt.figure(figsize=(8, 6))
sns.histplot(data=df, x="word_count", hue="sentiment", bins=50, kde=True)
plt.title("Distribui√ß√£o do N√∫mero de Palavras por Sentimento")
plt.xlabel("N√∫mero de Palavras")
plt.ylabel("Frequ√™ncia")
plt.show()

"""#### 8. An√°lise Avan√ßada de Bigramas & Trigramas"""

def get_top_ngrams(corpus, n=2, top_k=10):
    """
    Returns the top_k n-grams of length n from the corpus based on raw frequency.
    """
    vec = CountVectorizer(ngram_range=(n, n), stop_words=None)  # Use 'portuguese' if you want built-in stopwords
    bag_of_words = vec.fit_transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
    return words_freq[:top_k]

# Prepare corpus (lowercased reviews)
corpus = df["review"].apply(lambda x: x.lower()).tolist()

top_10_bigrams = get_top_ngrams(corpus, n=2, top_k=10)
top_10_trigrams = get_top_ngrams(corpus, n=3, top_k=10)

print("\n--- Top 10 Bigrams ---")
for phrase, freq in top_10_bigrams:
    print(f"{phrase}: {freq}")

print("\n--- Top 10 Trigrams ---")
for phrase, freq in top_10_trigrams:
    print(f"{phrase}: {freq}")

"""#### 9. Modelagem B√°sica de T√≥picos"""

# Convert text to a matrix of token counts
vectorizer = CountVectorizer(stop_words=None, max_features=5000)  # Adjust max_features as needed
X = vectorizer.fit_transform(corpus)

# Fit LDA with, for example, 5 topics
lda = LatentDirichletAllocation(n_components=5, random_state=42)
lda.fit(X)

# Print top words in each topic
def display_topics(model, feature_names, no_top_words=10):
    for idx, topic in enumerate(model.components_):
        print(f"\nTopic {idx + 1}:")
        top_features_idx = topic.argsort()[:-no_top_words - 1:-1]
        top_features = [feature_names[i] for i in top_features_idx]
        print(" ".join(top_features))

feature_names = vectorizer.get_feature_names_out()
display_topics(lda, feature_names, no_top_words=10)

"""#### 10. An√°lise de Correla√ß√£o entre Vari√°veis Num√©ricas"""

# We'll create a numeric subset of df for correlation
numeric_subset = df[["rating", "polarity", "word_count", "ttr"]].copy()

# Compute correlation matrix
corr_matrix = numeric_subset.corr()

print("\n--- Correlation Matrix ---")
print(corr_matrix)

# Visualize correlation matrix
plt.figure(figsize=(6, 5))
sns.heatmap(corr_matrix, annot=True, cmap="Blues", fmt=".2f")
plt.title("Correla√ß√£o entre Vari√°veis Num√©ricas")
plt.show()

"""# Divis√£o do Dataset

### Resumo

#### üß™ Divis√£o do Conjunto de Dados

#### üîÑ Separa√ß√£o em Dados de Treinamento e Teste

Para avaliar o desempenho dos modelos de classifica√ß√£o de forma justa, o conjunto de dados foi dividido em dois subconjuntos:

- **80%** para **treinamento**
- **20%** para **teste**

A divis√£o √© feita de forma aleat√≥ria, mas com um valor fixo de `random_state` para garantir reprodutibilidade dos experimentos. Essa separa√ß√£o permite treinar os modelos com uma por√ß√£o dos dados e, posteriormente, test√°-los com exemplos nunca vistos, garantindo uma avalia√ß√£o mais realista da generaliza√ß√£o do modelo.

### C√≥digo
"""

X_train, X_test, y_train, y_test = train_test_split(
    df["review"],
    df["polarity"],
    test_size=0.2,
    random_state=42
)

"""# Declara√ß√£o de Fun√ß√µes √öteis

### Resumo

#### ‚òÅÔ∏è Publica√ß√£o de Modelos no Hugging Face Hub

O projeto inclui utilit√°rios para salvar modelos treinados localmente e public√°-los diretamente no **Hugging Face Hub**, permitindo f√°cil reuso, versionamento e compartilhamento com a comunidade.

---

#### üíæ Upload de Modelos Keras e Transformers

Para modelos baseados em **TensorFlow/Keras** ou **Transformers**, √© utilizada a fun√ß√£o `save_model_to_hf()`, que realiza as seguintes etapas:

- Clona o reposit√≥rio do Hugging Face na m√°quina local
- Cria o arquivo `.gitattributes` para habilitar o uso do Git LFS (armazenamento de arquivos grandes)
- Salva o modelo no formato adequado:
  - `.keras` ou `.h5` para modelos Keras
  - `save_pretrained()` para modelos Transformers
- Realiza o commit e o push autom√°tico para o reposit√≥rio remoto no Hugging Face

A fun√ß√£o √© flex√≠vel, permitindo alternar entre modelos Keras e Transformers por meio do par√¢metro `is_transformer`.

---

#### üß† Upload de Modelos Scikit-learn

Para modelos treinados com **Scikit-learn**, √© utilizada a fun√ß√£o `save_sklearn_model_to_hf()` que:

- Serializa o modelo usando `joblib`
- Cria ou clona o reposit√≥rio desejado no Hugging Face Hub
- Copia o modelo para o diret√≥rio local do reposit√≥rio
- Realiza o commit e push autom√°tico para disponibilizar o modelo

Essa abordagem permite versionar e publicar modelos de classifica√ß√£o tradicionais (como SVM), mantendo a consist√™ncia com o restante do ecossistema Hugging Face.

---

Essas fun√ß√µes automatizam completamente o processo de publica√ß√£o de modelos, reduzindo o esfor√ßo manual e facilitando a reutiliza√ß√£o e o compartilhamento dos experimentos desenvolvidos no projeto.

### C√≥digo
"""

def save_model_to_hf(model, local_dir, file_name, repo_id, is_transformer=False):
    """
    Clones the repo into `local_dir`, creates a .gitattributes file to track large files (for Keras models),
    saves the model inside `local_dir`, and pushes to the HF Hub.

    Args:
      model: the trained model (a Keras model or a Transformers model)
      local_dir: local directory to clone the repository into (will be overwritten if it exists)
      file_name: the file name (for Keras models, e.g. "rnn_model.keras")
      repo_id: the Hugging Face repository ID (e.g., "layers2024/rnn-sentiment")
      is_transformer: if True, uses model.save_pretrained(); otherwise uses model.save()
    """
    subprocess.run(["git", "lfs", "install"], check=True)

    if os.path.exists(local_dir):
        shutil.rmtree(local_dir)

    create_repo(repo_id=repo_id, exist_ok=True, token=hf_token)

    repo = Repository(local_dir=local_dir, clone_from=repo_id, use_auth_token=hf_token)

    if not is_transformer:
        attributes_path = os.path.join(local_dir, ".gitattributes")
        with open(attributes_path, "w") as f:
            f.write("*.keras filter=lfs diff=lfs merge=lfs -text\n")
            f.write("*.h5 filter=lfs diff=lfs merge=lfs -text\n")
        repo.git_add(".gitattributes")
        repo.push_to_hub(commit_message="Add .gitattributes for Git LFS", blocking=True)

    save_path = os.path.join(local_dir, file_name)
    if not is_transformer:
        if not (save_path.endswith(".keras") or save_path.endswith(".h5")):
            save_path += ".keras"
        model.save(save_path)
    else:
        model.save_pretrained(save_path)

    repo.git_add()
    repo.push_to_hub(commit_message="Add trained model", blocking=True)

def save_sklearn_model_to_hf(model, file_path, repo_id):
    """
    Serializes a scikit-learn model using joblib and pushes it to the Hugging Face Hub.

    Args:
      model: The scikit-learn model to be saved.
      file_path: The local file path to save the model (e.g., "svm_sentencebert_model.joblib").
      repo_id: The Hugging Face repository ID (e.g., "layers2024/svm-sentencebert-sentiment").
    """
    dir_name = os.path.dirname(file_path)
    if dir_name and not os.path.exists(dir_name):
        os.makedirs(dir_name)

    joblib.dump(model, file_path)

    create_repo(repo_id=repo_id, exist_ok=True, token=hf_token)

    temp_dir = "temp_repo"
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir)

    repo = Repository(local_dir=temp_dir, clone_from=repo_id, use_auth_token=hf_token)

    shutil.copy(file_path, os.path.join(temp_dir, os.path.basename(file_path)))

    repo.git_add(os.path.basename(file_path))
    repo.push_to_hub(commit_message="Save scikit-learn model", blocking=True)

"""# Classifica√ß√£o com Modelos Transformers e Embeddings Contextuais

## BERT para Classifica√ß√£o

### Resumo

#### ü§ñ Classifica√ß√£o de Sentimentos com BERT (Portugu√™s)

Nesta etapa, o projeto utiliza o modelo **BERT pr√©-treinado para portugu√™s** da Neuralmind, hospedado no Hugging Face Hub, para classificar sentimentos em avalia√ß√µes de consumidores.

---

#### üî† Tokeniza√ß√£o e Prepara√ß√£o dos Dados
- As avalia√ß√µes de treino e teste s√£o tokenizadas com `BertTokenizer`, utilizando padding e truncamento autom√°ticos.
- As entradas s√£o convertidas em tensores compat√≠veis com o TensorFlow para alimentar o modelo BERT.

---

#### ‚öñÔ∏è Balanceamento de Classes
Para lidar com o desbalanceamento entre avalia√ß√µes positivas e negativas, s√£o calculados **pesos de classe** com base na distribui√ß√£o do conjunto de treino. Esses pesos s√£o aplicados manualmente na fun√ß√£o de perda para penalizar classes minorit√°rias de forma adequada.

---

#### üß† Treinamento do Modelo
O modelo `TFBertForSequenceClassification` √© treinado utilizando:
- Otimizador **AdamWeightDecay**
- Fun√ß√£o de perda ponderada personalizada
- M√©trica de avalia√ß√£o `SparseCategoricalAccuracy`
- Ciclo de treinamento implementado manualmente com `GradientTape`, proporcionando mais controle sobre o processo

A quantidade de √©pocas e o tamanho dos lotes s√£o definidos por par√¢metros globais (`DEFAULT_EPOCHS`, `DEFAULT_BATCH_SIZE`).

---

#### üß™ Avalia√ß√£o do Modelo
Ap√≥s o treinamento, o modelo √© avaliado com os dados de teste. As m√©tricas de avalia√ß√£o incluem:
- **Loss** (fun√ß√£o de perda)
- **Acur√°cia**
- **F1 Score (ponderado)**

Al√©m disso, √© exibida uma **matriz de confus√£o** para visualizar a performance do modelo em rela√ß√£o √† polaridade (positivo/negativo).

---

#### ‚òÅÔ∏è Publica√ß√£o no Hugging Face Hub
Ao final do processo, o modelo BERT treinado √© salvo e publicado no **Hugging Face Hub** atrav√©s da fun√ß√£o `save_model_to_hf()`, garantindo que ele possa ser reutilizado e compartilhado com a comunidade. O upload utiliza o Git LFS para arquivos grandes e segue as boas pr√°ticas de versionamento.

---

Este modelo BERT representa uma abordagem moderna e poderosa para tarefas de **classifica√ß√£o bin√°ria de sentimentos**, aproveitando o poder dos LLMs com o suporte completo √† l√≠ngua portuguesa.

### C√≥digo
"""

tokenizer = BertTokenizer.from_pretrained("neuralmind/bert-base-portuguese-cased")
model = TFBertForSequenceClassification.from_pretrained(
    "neuralmind/bert-base-portuguese-cased",
    num_labels=2  # binary: negativo (0), positivo (1)
)

X_train_tokens = tokenizer(list(X_train), padding=True, truncation=True, return_tensors="tf")
X_test_tokens = tokenizer(list(X_test), padding=True, truncation=True, return_tensors="tf")

counts = np.array([
    np.sum(y_train == 0),  # negativo
    np.sum(y_train == 1)   # positivo
])
total = counts.sum()
class_weights = {i: total / count for i, count in enumerate(counts)}
print("Class weights:", class_weights)

def weighted_loss(y_true, y_pred):
    losses = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)
    weights_tensor = tf.constant([class_weights[0], class_weights[1]], dtype=tf.float32)
    sample_weights = tf.gather(weights_tensor, tf.cast(y_true, tf.int32))
    return tf.reduce_mean(losses * sample_weights)

optimizer = AdamWeightDecay(learning_rate=3e-5)
metric = tf.keras.metrics.SparseCategoricalAccuracy()

@tf.function
def train_step(inputs, targets):
    with tf.GradientTape() as tape:
        predictions = model(inputs)
        loss = weighted_loss(targets, predictions.logits)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    metric.update_state(targets, predictions.logits)
    return loss, metric.result()

y_train_array = y_train.to_numpy()

epochs = DEFAULT_EPOCHS
batch_size = DEFAULT_BATCH_SIZE

for epoch in range(epochs):
    print(f"\nEpoch {epoch + 1}/{epochs}")
    metric.reset_state()
    for i in range(0, len(X_train), batch_size):
        batch_inputs = {k: v[i:i + batch_size] for k, v in X_train_tokens.data.items()}
        batch_targets = y_train_array[i:i + batch_size]
        loss, accuracy = train_step(batch_inputs, batch_targets)
        print(f"Batch {i // batch_size + 1}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}")

"""#### Avalia√ß√£o dos Resultados para o BERT"""

batch_size = 32 if 'DEFAULT_BATCH_SIZE' not in globals() else DEFAULT_BATCH_SIZE
all_predictions = []

for i in range(0, len(X_test), batch_size):
    batch_inputs = {k: v[i:i + batch_size] for k, v in X_test_tokens.data.items()}
    batch_logits = model(batch_inputs).logits
    all_predictions.append(batch_logits)

predictions = tf.concat(all_predictions, axis=0)
predicted_labels = np.argmax(predictions, axis=1)

y_test_adjusted = y_test.values

loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
loss = loss_fn(y_test_adjusted, predictions).numpy()

accuracy_bert = accuracy_score(y_test_adjusted, predicted_labels)
f1_bert = f1_score(y_test_adjusted, predicted_labels, average='weighted')

print("\nBERT:")
print(f"Test Loss: {loss:.4f}")
print(f"Acur√°cia: {accuracy_bert:.4f}")
print(f"F1 Score: {f1_bert:.4f}")

"""#### Matriz de Confus√£o para o BERT"""

def plot_confusion_matrix(y_true, y_pred, title="Confusion Matrix"):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=["negativo", "positivo"],
                yticklabels=["negativo", "positivo"])
    plt.title(title)
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.show()

plot_confusion_matrix(y_test_adjusted, predicted_labels, title="Confusion Matrix - BERT (Polarity)")

print("Saving BERT model to HF Hub...")
save_model_to_hf(
    model=model,
    local_dir="bert_model",
    file_name="",
    repo_id="layers2024/bert-sentiment",
    is_transformer=True
)
print("BERT model successfully pushed to HF Hub!")

"""## DistilBERT para Classifica√ß√£o

### Resumo

#### üåç Classifica√ß√£o de Sentimentos com DistilBERT Multil√≠ngue

Al√©m do BERT em portugu√™s, o projeto tamb√©m explora o uso do **DistilBERT multil√≠ngue**, uma vers√£o leve e eficiente de BERT que suporta m√∫ltiplos idiomas, incluindo o portugu√™s.

---

#### üî† Tokeniza√ß√£o e Prepara√ß√£o dos Dados
- Os dados s√£o processados com o `DistilBertTokenizer`, convertendo os textos em tensores de entrada apropriados para o modelo.
- Padding e truncamento s√£o aplicados automaticamente para lidar com varia√ß√µes no comprimento dos textos.

---

#### ‚öñÔ∏è Balanceamento de Classes
Assim como no BERT, s√£o calculados **pesos de classe** com base na propor√ß√£o de exemplos positivos e negativos no dataset. Esses pesos s√£o aplicados na fun√ß√£o de perda para garantir um treinamento mais justo.

---

#### üß† Treinamento do Modelo
O modelo `TFDistilBertForSequenceClassification` √© treinado com:
- Otimizador **AdamWeightDecay**
- Fun√ß√£o de perda personalizada ponderada
- M√©trica de acur√°cia com `SparseCategoricalAccuracy`
- Treinamento manual com `GradientTape`, permitindo controle completo sobre os passos de backpropagation

O treinamento √© executado com o n√∫mero de √©pocas e tamanho de batch definidos pelos par√¢metros globais do projeto.

---

#### üß™ Avalia√ß√£o do Modelo
Ap√≥s o treinamento, o modelo DistilBERT √© testado e avaliado com base em:

- **Acur√°cia**
- **F1 Score ponderado**
- **Relat√≥rio de classifica√ß√£o** completo
- **Matriz de confus√£o** para visualiza√ß√£o dos acertos e erros por classe

Essas m√©tricas permitem comparar seu desempenho com outras abordagens, como o modelo BERT em portugu√™s.

---

#### ‚òÅÔ∏è Publica√ß√£o no Hugging Face Hub
Ao final do processo, o modelo treinado √© salvo e publicado automaticamente no **Hugging Face Hub**. Essa integra√ß√£o facilita a reutiliza√ß√£o do modelo em outros projetos e experimentos, al√©m de permitir o versionamento do progresso.

---

O uso do **DistilBERT multil√≠ngue** permite avaliar a capacidade de generaliza√ß√£o de modelos mais leves em contextos reais de classifica√ß√£o de sentimentos, com √≥timo custo-benef√≠cio computacional.

### C√≥digo
"""

tokenizer_distil = DistilBertTokenizer.from_pretrained("distilbert-base-multilingual-cased")
model_distil = TFDistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-multilingual-cased", num_labels=2  # binary: negativo vs positivo
)

X_train_tokens_distil = tokenizer_distil(list(X_train), padding=True, truncation=True, return_tensors="tf")
X_test_tokens_distil = tokenizer_distil(list(X_test), padding=True, truncation=True, return_tensors="tf")

optimizer_distil = AdamWeightDecay(learning_rate=3e-5)
metric_distil = tf.keras.metrics.SparseCategoricalAccuracy()

counts = np.array([
    df[df["polarity"] == 0].shape[0],  # negativo
    df[df["polarity"] == 1].shape[0]   # positivo
])
total = counts.sum()
class_weights = {i: total / count for i, count in enumerate(counts)}
print("Class weights:", class_weights)

weights_tensor = tf.constant([class_weights[0], class_weights[1]], dtype=tf.float32)

def weighted_loss(y_true, y_pred):
    losses = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)
    sample_weights = tf.gather(weights_tensor, tf.cast(y_true, tf.int32))
    return tf.reduce_mean(losses * sample_weights)

def train_step_distil(inputs, targets):
    with tf.GradientTape() as tape:
        outputs = model_distil(inputs)
        logits = outputs.logits
        loss = weighted_loss(targets, logits)
    gradients = tape.gradient(loss, model_distil.trainable_variables)
    optimizer_distil.apply_gradients(zip(gradients, model_distil.trainable_variables))
    metric_distil.update_state(targets, logits)
    return loss, metric_distil.result()

epochs = DEFAULT_EPOCHS
batch_size = DEFAULT_BATCH_SIZE

print("Treinando o DistilBERT...")
for epoch in range(epochs):
    print(f"\nEpoch {epoch + 1}/{epochs}")
    metric_distil.reset_state()
    for i in range(0, len(X_train), batch_size):
        batch_inputs = {k: v[i:i + batch_size] for k, v in X_train_tokens_distil.data.items()}
        batch_targets = y_train.values[i:i + batch_size]
        loss, acc = train_step_distil(batch_inputs, batch_targets)
        if (i // batch_size) % 10 == 0:
            print(f"  Batch {(i // batch_size) + 1}: Loss = {loss:.4f}, Accuracy = {acc:.4f}")

"""#### Avalia√ß√£o do Resultados para o DistilBERT"""

print("\nAvaliando o DistilBERT no conjunto de teste...")

all_preds = []
for i in range(0, len(X_test), batch_size):
    batch_inputs = {k: v[i:i + batch_size] for k, v in X_test_tokens_distil.data.items()}
    logits = model_distil(batch_inputs).logits
    all_preds.append(logits)

predictions = tf.concat(all_preds, axis=0)
predicted_labels = np.argmax(predictions, axis=1)

y_test_adjusted = y_test.values

accuracy_distil = accuracy_score(y_test_adjusted, predicted_labels)
f1_distil = f1_score(y_test_adjusted, predicted_labels, average='weighted')

print("\nDistilBERT:")
print(f"Accuracy: {accuracy_distil:.4f}")
print(f"F1 Score: {f1_distil:.4f}")
print("\nClassification Report:")
print(classification_report(y_test_adjusted, predicted_labels, target_names=["negativo", "positivo"]))

"""#### Matriz de Confus√£o para o DistilBERT"""

cm = confusion_matrix(y_test_adjusted, predicted_labels)

plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=["negativo", "positivo"],
            yticklabels=["negativo", "positivo"])
plt.title("Confusion Matrix - DistilBERT (Polarity)")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

print("Saving DistilBERT model to HF Hub...")
save_model_to_hf(
    model=model_distil,
    local_dir="distilbert_model",
    file_name="",
    repo_id="layers2024/distilbert-sentiment",
    is_transformer=True
)
print("DistilBERT model successfully pushed to HF Hub!")

"""## RoBERTa para Classifica√ß√£o

### Resumo

#### üåê Classifica√ß√£o de Sentimentos com XLM-RoBERTa

O projeto tamb√©m utiliza o **XLM-RoBERTa**, uma variante robusta e multil√≠ngue do modelo RoBERTa, com suporte aprimorado para m√∫ltiplos idiomas, incluindo o portugu√™s. Essa abordagem permite avaliar o desempenho de um modelo pr√©-treinado em escala global para an√°lise de sentimentos em dados nacionais.

---

#### üî† Tokeniza√ß√£o e Prepara√ß√£o dos Dados
- As avalia√ß√µes de entrada s√£o tokenizadas com o `XLMRobertaTokenizer`.
- A tokeniza√ß√£o inclui padding, truncamento e um limite de tamanho (`max_length`) para padronizar a entrada.
- Os dados s√£o convertidos para tensores compat√≠veis com o TensorFlow.

---

#### ‚öñÔ∏è Balanceamento de Classes
O modelo aplica **pesos de classe din√¢micos** calculados com base na distribui√ß√£o de sentimentos positivos e negativos no dataset. Isso assegura que o modelo n√£o favore√ßa a classe majorit√°ria durante o treinamento.

---

#### üíª Configura√ß√£o de GPU
Antes do treinamento, o projeto ativa o crescimento de mem√≥ria nas GPUs dispon√≠veis (se houver) para evitar aloca√ß√£o total e permitir uso mais eficiente do hardware.

---

#### üß† Treinamento do Modelo
O modelo `TFXLMRobertaForSequenceClassification` √© treinado com:
- Otimizador **AdamWeightDecay**
- Fun√ß√£o de perda customizada com pesos de classe
- Acompanhamento da acur√°cia com `SparseCategoricalAccuracy`
- Loop de treinamento manual utilizando `GradientTape` para controle total

O treinamento ocorre em batches definidos pelo par√¢metro global `DEFAULT_BATCH_SIZE`, por um n√∫mero de √©pocas determinado.

---

#### üß™ Avalia√ß√£o do Modelo
Ap√≥s o treinamento, o modelo XLM-RoBERTa √© avaliado com os dados de teste:
- **Acur√°cia**
- **F1 Score (ponderado)**
- **Relat√≥rio de classifica√ß√£o** com m√©tricas detalhadas para cada classe
- **Matriz de confus√£o** visual, destacando os acertos e erros de classifica√ß√£o por polaridade

---

#### ‚òÅÔ∏è Publica√ß√£o no Hugging Face Hub
O modelo treinado √© exportado e publicado automaticamente no **Hugging Face Hub**, utilizando Git LFS para suporte a arquivos grandes. Isso garante que o modelo possa ser reutilizado e compartilhado com a comunidade de forma simples e eficaz.

---

Essa etapa demonstra a capacidade de modelos multil√≠ngues de capturar nuances lingu√≠sticas em portugu√™s, oferecendo uma solu√ß√£o escal√°vel e eficiente para an√°lise de sentimentos em ambientes multiculturais.

### C√≥digo
"""

counts = np.array([
    df[df["polarity"] == 0].shape[0],  # negativo
    df[df["polarity"] == 1].shape[0]   # positivo
])
total = counts.sum()

class_weights = {i: total / count for i, count in enumerate(counts)}
print("Class weights:", class_weights)

gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# Tokenizer and model (binary classification)
tokenizer_xlm = XLMRobertaTokenizer.from_pretrained("xlm-roberta-base")
model_roberta = TFXLMRobertaForSequenceClassification.from_pretrained("xlm-roberta-base", num_labels=2)

# X_train_tokens_roberta = tokenizer_xlm(list(X_train), padding=True, truncation=True, return_tensors="tf")
X_train_tokens_roberta = tokenizer_xlm(list(X_train), padding='max_length', truncation=True, max_length=128, return_tensors="tf")
X_test_tokens_roberta = tokenizer_xlm(list(X_test), padding=True, truncation=True, return_tensors="tf")

optimizer_roberta = AdamWeightDecay(learning_rate=3e-5)
metric_roberta = tf.keras.metrics.SparseCategoricalAccuracy()

counts = np.array([
    df[df["polarity"] == 0].shape[0],
    df[df["polarity"] == 1].shape[0]
])
total = counts.sum()
class_weights = {i: total / count for i, count in enumerate(counts)}
weights_tensor = tf.constant([class_weights[0], class_weights[1]], dtype=tf.float32)

def weighted_loss_roberta(y_true, y_pred):
    losses = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)
    sample_weights = tf.gather(weights_tensor, tf.cast(y_true, tf.int32))
    return tf.reduce_mean(losses * sample_weights)

@tf.function
def train_step_roberta(inputs, targets):
    with tf.GradientTape() as tape:
        outputs = model_roberta(inputs)
        logits = outputs.logits
        loss = weighted_loss_roberta(targets, logits)
    gradients = tape.gradient(loss, model_roberta.trainable_variables)
    optimizer_roberta.apply_gradients(zip(gradients, model_roberta.trainable_variables))
    metric_roberta.update_state(targets, logits)
    return loss, metric_roberta.result()

epochs = DEFAULT_EPOCHS
batch_size = DEFAULT_BATCH_SIZE #8

print("Treinando o RoBERTa...")
for epoch in range(epochs):
    print(f"\nEpoch {epoch+1}/{epochs}")
    metric_roberta.reset_state()
    for i in range(0, len(X_train), batch_size):
        batch_inputs = {k: v[i:i+batch_size] for k, v in X_train_tokens_roberta.data.items()}
        batch_targets = y_train.values[i:i+batch_size]
        loss, acc = train_step_roberta(batch_inputs, batch_targets)
        if (i // batch_size) % 10 == 0:
            print(f"  Batch {(i // batch_size)+1}: Loss = {loss:.4f}, Accuracy = {acc:.4f}")

"""#### Avalia√ß√£o de Resultados para o RoBERTa"""

print("\nAvaliando o RoBERTa no conjunto de teste...")

all_preds = []
for i in range(0, len(X_test), batch_size):
    batch_inputs = {k: v[i:i+batch_size] for k, v in X_test_tokens_roberta.data.items()}
    logits = model_roberta(batch_inputs).logits
    all_preds.append(logits)

predictions_roberta = tf.concat(all_preds, axis=0)
predicted_labels_roberta = np.argmax(predictions_roberta, axis=1)

y_test_adjusted = y_test.values

accuracy_roberta = accuracy_score(y_test_adjusted, predicted_labels_roberta)
f1_roberta = f1_score(y_test_adjusted, predicted_labels_roberta, average='weighted')

print("\nRoBERTa:")
print(f"Accuracy: {accuracy_roberta:.4f}")
print(f"F1 Score: {f1_roberta:.4f}")
print("\nClassification Report:")
print(classification_report(y_test_adjusted, predicted_labels_roberta, target_names=["negativo", "positivo"]))

"""#### Matriz de Confus√£o para o RoBERTa"""

cm = confusion_matrix(y_test_adjusted, predicted_labels_roberta)

plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=["negativo", "positivo"],
            yticklabels=["negativo", "positivo"])
plt.title("Confusion Matrix - RoBERTa (Polarity)")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

print("Saving RoBERTa model to HF Hub...")
save_model_to_hf(
    model=model_roberta,
    local_dir="roberta_model",
    file_name="",
    repo_id="layers2024/roberta-sentiment",
    is_transformer=True
)
print("RoBERTa model successfully pushed to HF Hub!")

"""## SVM + Sentence-BERT para Classifica√ß√£o

### Resumo

#### üß† Classifica√ß√£o com Sentence-BERT + SVM

Al√©m dos modelos Transformers fine-tunados, o projeto tamb√©m utiliza uma abordagem h√≠brida mais leve combinando **Sentence-BERT** para extra√ß√£o de embeddings sem√¢nticos com um **classificador SVM** tradicional.

---

#### üß© Gera√ß√£o de Embeddings com Sentence-BERT
O modelo `distiluse-base-multilingual-cased-v1`, da biblioteca **Sentence-Transformers**, √© utilizado para transformar os textos das avalia√ß√µes em vetores num√©ricos que capturam o significado sem√¢ntico de cada coment√°rio.

- Os embeddings s√£o gerados para os conjuntos de treino e teste.
- Essa etapa √© eficiente e dispensa o fine-tuning completo de um modelo Transformer.

---

#### üß™ Classifica√ß√£o com SVM
Com os embeddings gerados, √© treinado um modelo **Support Vector Machine (SVM)** utilizando o Scikit-learn:

- O classificador √© configurado com `class_weight="balanced"` para compensar poss√≠veis desbalanceamentos no dataset.
- Ap√≥s o treinamento, o modelo √© testado com os embeddings do conjunto de teste.

---

#### üìä Avalia√ß√£o do Modelo
A performance do SVM √© avaliada com base em:
- **Relat√≥rio de classifica√ß√£o completo** com precis√£o, recall e F1 score por classe
- **Acur√°cia geral**
- **F1 Score ponderado**
- **Matriz de confus√£o** para visualizar os acertos e erros na polaridade prevista

---

#### ‚òÅÔ∏è Publica√ß√£o no Hugging Face Hub
O modelo SVM treinado √© serializado com `joblib` e publicado no **Hugging Face Hub** atrav√©s da fun√ß√£o `save_sklearn_model_to_hf()`.

Essa abordagem oferece uma alternativa eficiente e leve aos modelos Transformers completos, ideal para aplica√ß√µes com restri√ß√µes de recursos computacionais.

---

O uso de Sentence-BERT com SVM demonstra a efic√°cia de modelos baseados em embeddings prontos para tarefas de classifica√ß√£o bin√°ria, com excelente equil√≠brio entre desempenho e efici√™ncia.

### C√≥digo
"""

svm_sbert = SVC(class_weight="balanced")

y_train_sbert = y_train.values
y_test_sbert = y_test.values

model_sbert = SentenceTransformer("distiluse-base-multilingual-cased-v1")
print("Modelo Sentence-BERT carregado com sucesso.")

print("Gerando embeddings para os dados de treino...")
X_train_sbert = model_sbert.encode(X_train.tolist(), show_progress_bar=True)

print("Gerando embeddings para os dados de teste...")
X_test_sbert = model_sbert.encode(X_test.tolist(), show_progress_bar=True)

svm_sbert = SVC(class_weight="balanced")
svm_sbert.fit(X_train_sbert, y_train_sbert)

y_pred_sbert = svm_sbert.predict(X_test_sbert)

"""#### Avalia√ß√£o de Resultados para o SVM + Sentence-BERT"""

print("\nRelat√≥rio de Classifica√ß√£o para Sentence-BERT + SVM:")
print(classification_report(y_test_sbert, y_pred_sbert, target_names=["negativo", "positivo"]))

accuracy_sbert = accuracy_score(y_test_sbert, y_pred_sbert)
f1_sbert = f1_score(y_test_sbert, y_pred_sbert, average='weighted')

print(f"Accuracy: {accuracy_sbert:.4f}")
print(f"F1 Score: {f1_sbert:.4f}")

"""#### Matriz de Confus√£o para o SVM + Sentence-BERT"""

cm = confusion_matrix(y_test_sbert, y_pred_sbert)

plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=["negativo", "positivo"],
            yticklabels=["negativo", "positivo"])
plt.title("Confusion Matrix - Sentence-BERT + SVM (Polarity)")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

save_sklearn_model_to_hf(
    model=svm_sbert,
    file_path="svm_sentencebert_model.joblib",
    repo_id="layers2024/svm-sentencebert-sentiment"
)

"""## SVM + Universal Sentence Encoder (USE) para Classifica√ß√£o

### Resumo

#### üî° Classifica√ß√£o com Universal Sentence Encoder (USE) + SVM

O projeto tamb√©m explora o uso do **Universal Sentence Encoder Multil√≠ngue (USE)** em conjunto com um classificador **SVM**, oferecendo uma solu√ß√£o leve, eficiente e sem necessidade de fine-tuning para a tarefa de classifica√ß√£o de sentimentos.

---

#### üåê Gera√ß√£o de Embeddings com USE
O modelo `universal-sentence-encoder-multilingual/3`, hospedado no **TensorFlow Hub**, √© utilizado para transformar as avalia√ß√µes textuais em vetores num√©ricos de alta qualidade sem√¢ntica.

- Os embeddings s√£o gerados para os conjuntos de treino e teste.
- O processo √© simples, r√°pido e compat√≠vel com m√∫ltiplos idiomas.

---

#### ‚öñÔ∏è Padroniza√ß√£o dos Embeddings
Antes do treinamento, os vetores gerados s√£o padronizados com `StandardScaler` para garantir melhor desempenho do classificador SVM.

---

#### üß™ Classifica√ß√£o com SVM
Um **Support Vector Machine (SVM)** com `class_weight="balanced"` √© treinado com os embeddings do USE para prever a polaridade das avalia√ß√µes.

Essa abordagem tradicional oferece bons resultados com baixo custo computacional, ideal para cen√°rios com restri√ß√µes de recursos.

---

#### üìä Avalia√ß√£o do Modelo
O desempenho da combina√ß√£o **USE + SVM** √© avaliado por meio de:

- **Acur√°cia geral**
- **F1 Score ponderado**
- **Relat√≥rio de classifica√ß√£o** com m√©tricas por classe
- **Matriz de confus√£o** com visualiza√ß√£o das previs√µes corretas e incorretas

---

#### ‚òÅÔ∏è Publica√ß√£o no Hugging Face Hub
Ap√≥s o treinamento, o modelo SVM √© salvo e publicado no **Hugging Face Hub** com o nome `svm-use-sentiment`, permitindo reutiliza√ß√£o futura e compartilhamento com a comunidade.

---

O uso do **USE + SVM** mostra como √© poss√≠vel obter **bons resultados com solu√ß√µes leves**, sem a necessidade de treinar modelos grandes ou ajustar hiperpar√¢metros complexos, o que torna essa abordagem uma excelente alternativa pr√°tica e acess√≠vel.

### C√≥digo
"""

use_model_url = "https://tfhub.dev/google/universal-sentence-encoder-multilingual/3"
use_model = hub.load(use_model_url)
print("Universal Sentence Encoder loaded.")

def get_use_embeddings(texts):
    embeddings = use_model(texts)
    return embeddings.numpy()

print("Gerando embeddings para os dados de treino...")
X_train_use = get_use_embeddings(X_train.tolist())
print("Gerando embeddings para os dados de teste...")
X_test_use = get_use_embeddings(X_test.tolist())

scaler = StandardScaler()
X_train_use = scaler.fit_transform(X_train_use)
X_test_use = scaler.transform(X_test_use)

y_train_use = y_train.values
y_test_use = y_test.values

svm_use = SVC(class_weight="balanced")
svm_use.fit(X_train_use, y_train_use)
y_pred_use = svm_use.predict(X_test_use)

"""#### Avalia√ß√£o de Resultados para o SVM + Universal Sentence Encoder (USE)"""

print("\nRelat√≥rio de Classifica√ß√£o para USE + SVM:")
print(classification_report(y_test, y_pred_use, target_names=["negativo", "positivo"]))

accuracy_use = accuracy_score(y_test, y_pred_use)
f1_use = f1_score(y_test, y_pred_use, average='weighted')

print(f"Accuracy: {accuracy_use:.4f}")
print(f"F1 Score: {f1_use:.4f}")

"""#### Matriz de Confus√£o para o SVM + Universal Sentence Encoder (USE)"""

# Convert numeric polarity values to string labels for visualization
label_map = {0: "negativo", 1: "positivo"}
y_test_str = [label_map[r] for r in y_test]
y_pred_use_str = [label_map[r] for r in y_pred_use]

# Generate and plot the binary confusion matrix
cm = confusion_matrix(y_test_str, y_pred_use_str, labels=["negativo", "positivo"])
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=["negativo", "positivo"],
            yticklabels=["negativo", "positivo"])
plt.title("Confusion Matrix - USE + SVM (Polarity)")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

save_sklearn_model_to_hf(
    model=svm_use,
    file_path="svm_use_model.joblib",
    repo_id="layers2024/svm-use-sentiment"
)

"""# Comparativo de M√©tricas e Resultados

### Resumo

#### üìà Compara√ß√£o de Desempenho dos Modelos

Ao final do experimento, os principais modelos utilizados no projeto s√£o comparados com base em suas m√©tricas de desempenho ‚Äî **Acur√°cia** e **F1-Score**. Essa an√°lise permite identificar quais abordagens foram mais eficazes na tarefa de classifica√ß√£o de sentimentos.

---

#### üß™ Modelos Avaliados

Os seguintes modelos foram comparados:

- **BERT** (Portugu√™s - fine-tuned)
- **DistilBERT** (Multil√≠ngue - fine-tuned)
- **XLM-RoBERTa** (Multil√≠ngue - fine-tuned)
- **SVM + Sentence-BERT** (embeddings sem√¢nticos + classificador leve)
- **SVM + USE** (Universal Sentence Encoder + SVM)

---

#### üìä M√©tricas Utilizadas

- **Acur√°cia**: propor√ß√£o total de classifica√ß√µes corretas.
- **F1-Score ponderado**: equil√≠brio entre precis√£o e recall, levando em conta o desbalanceamento entre classes.

---

#### üìâ Visualiza√ß√£o dos Resultados

Os resultados s√£o apresentados em um **gr√°fico de barras**, comparando lado a lado as pontua√ß√µes de acur√°cia e F1-Score para cada modelo.

Essa visualiza√ß√£o facilita a interpreta√ß√£o das performances relativas e ajuda na escolha do modelo mais adequado conforme os crit√©rios de efici√™ncia, precis√£o e custo computacional.

---

Essa an√°lise consolidada refor√ßa a import√¢ncia de testar m√∫ltiplas abordagens, combinando modelos de √∫ltima gera√ß√£o com solu√ß√µes mais leves, para encontrar o melhor equil√≠brio entre **desempenho e efici√™ncia** em projetos reais de PLN.

### C√≥digo
"""

# Cria um dicion√°rio com os resultados
resultados = {
    "Modelo": ["BERT", "DistilBERT", "RoBERTa", "SVM + Sentence-BERT", "SVM + USE"],
    "Acur√°cia": [accuracy_bert, accuracy_distil, accuracy_roberta, accuracy_sbert, accuracy_use],
    "F1-Score": [f1_bert, f1_distil, f1_roberta, f1_sbert, f1_use]
}

# Cria um DataFrame pandas com os resultados
df_resultados = pd.DataFrame(resultados)

# Exibe o DataFrame
print(df_resultados)

"""#### Gr√°fico com Compara√ß√£o de Desempenho dos Modelos"""

plt.figure(figsize=(14, 6))
# Turn off the automatic legend by passing legend=False
df_resultados.plot(x="Modelo", y=["Acur√°cia", "F1-Score"], kind="bar",
                   width=0.6, color=["skyblue", "lightcoral"], legend=False)

plt.title("Compara√ß√£o de Desempenho dos Modelos")
plt.ylabel("Pontua√ß√£o")
plt.xticks(rotation=45, ha="right")

# Manually add the legend outside the plot (to the right)
plt.legend(["Acur√°cia", "F1-Score"], title="M√©trica",
           bbox_to_anchor=(1.05, 1), loc="upper left")

plt.tight_layout()
plt.show()

"""# Classifica√ß√£o com In-Context Learning

### Resumo

#### ü§ñ Classifica√ß√£o com Modelos de Linguagem Generativa (LLMs)

Nesta etapa, o projeto avalia a capacidade de modelos generativos de linguagem (LLMs) ‚Äî **OpenAI GPT-4** e **Google Gemini 1.5** ‚Äî em classificar avalia√ß√µes de produtos como **"positivo"** ou **"negativo"**, sem qualquer ajuste fino (zero-shot).

---

#### üßæ Prompt de Instru√ß√£o
Ambos os modelos recebem o mesmo prompt estruturado que simula um cen√°rio de assistente virtual de an√°lise de sentimentos. O prompt √© projetado para solicitar uma resposta clara e objetiva, sem explica√ß√µes adicionais.

---

#### üß™ Execu√ß√£o da Classifica√ß√£o
- 10 avalia√ß√µes aleat√≥rias s√£o selecionadas para teste com cada modelo.
- A resposta de cada modelo √© comparada com o sentimento real rotulado (`positivo` ou `negativo`).
- Emojis e indicadores visuais s√£o adicionados √† classifica√ß√£o para facilitar a leitura dos resultados.

---

#### ‚úÖ Avalia√ß√£o de Desempenho

As m√©tricas calculadas para cada modelo incluem:
- **Acur√°cia global** (% de classifica√ß√µes corretas)
- **Total de acertos e erros**
- **Acur√°cia por categoria** (positivo / negativo)
- **Relat√≥rio de classifica√ß√£o (precision, recall, F1-Score)**
- **Matriz de confus√£o**

Essas m√©tricas oferecem uma vis√£o detalhada sobre a performance dos modelos tanto no geral quanto por classe.

---

#### üìä Visualiza√ß√µes
S√£o gerados diversos gr√°ficos comparativos:
- **Gr√°fico de barras** com a acur√°cia geral de cada modelo
- **Gr√°fico por categoria**, exibindo a acur√°cia espec√≠fica para sentimentos positivos e negativos
- **Matrizes de confus√£o** lado a lado para OpenAI GPT-4 e Google Gemini 1.5

Essas visualiza√ß√µes ajudam a entender n√£o apenas quem acerta mais, mas **onde** cada modelo tem maior ou menor dificuldade.

---

Essa etapa demonstra a flexibilidade e poder dos LLMs modernos, permitindo resolver tarefas de PLN com precis√£o competitiva e **sem necessidade de treinar modelos complexos**, tornando-os ideais para prototipagem r√°pida e solu√ß√µes de baixo custo operacional.

### C√≥digo

#### Declara√ß√£o do Prompt e Fun√ß√µes √öteis
"""

# Define o temeplate do prompt
prompt_template = """
Voc√™ √© um assistente de an√°lise de sentimentos. Sua tarefa √© classificar avalia√ß√µes de produtos como "positivo" ou "negativo".

Aqui est√° uma avalia√ß√£o de um produto:
{review_text}

Classifique a avalia√ß√£o acima como "Positivo" ou "Negativo". Responda apenas com a classifica√ß√£o, sem adicionar coment√°rios ou explica√ß√µes adicionais.
"""

# Define fun√ß√£o para exibi√ß√£o da classifica√ß√£o
def add_emoji_to_classification(classification):
  if classification == "Positivo":
    return "üòä " + classification
  elif classification == "Negativo":
    return "üòû " + classification
  else:
    return classification

# Define fun√ß√£o para compara√ß√£o com resultado da coluna "sentiment"
def compare_classification_with_actual(classification, actual_sentiment):
  is_correct = classification.lower() == actual_sentiment.lower()
  result = "‚úÖ Correto" if is_correct else "‚ùå Incorreto"
  return result, is_correct

"""#### In-Context Learning com OpenAI (GPT-4o)"""

llm = ChatOpenAI(
    model_name="gpt-4o",
    temperature=0
)

def classify_with_openai(review_text):
    prompt = prompt_template.format(review_text=review_text)

    response = llm.predict(prompt)
    return response.strip()

random_reviews = df.sample(n=10)
openai_results = []

print("Classifica√ß√£o da OpenAI GPT-4")
print("=" * 90)

for index, row in random_reviews.iterrows():
    review_text = row['review']
    classification = classify_with_openai(review_text)
    display_classification = add_emoji_to_classification(classification)
    result, is_correct = compare_classification_with_actual(classification, row['sentiment'])

    openai_results.append({
        'Review': review_text,
        'Sentiment': row['sentiment'],
        'Classification': classification,
        'Display Classification': display_classification,
        'Result': result,
        'Is Correct': is_correct
    })

    print(f"\nAvalia√ß√£o: {review_text}")
    print(f"Classifica√ß√£o da OpenAI: {display_classification}")
    print("-" * 90)
    print(f"Sentimento Real: {row['sentiment'].capitalize()}")
    print(f"Resultado: {result}")

"""#### In-Context Learning com Google Gemini 1.5"""

genai.configure(api_key=myKey)

# Espcifica o modelo do Gemini
model = genai.GenerativeModel("gemini-1.5-flash-latest")

def classify_with_gemini(review_text):
    response = model.generate_content(prompt_template.format(review_text=review_text))
    classification = response.text.strip()
    return classification

# Classifica 10 reviews aleat√≥rias
random_reviews = df.sample(n=10)
gemini_results = []  # Armazena os resultados do Gemini

print("Classifica√ß√£o do Google Gemini 1.5")
print("=" * 90)

for index, row in random_reviews.iterrows():
    review_text = row['review']
    classification = classify_with_gemini(review_text)
    display_classification = add_emoji_to_classification(classification)
    result, is_correct = compare_classification_with_actual(classification, row['sentiment'])

    gemini_results.append({
        'Review': review_text,
        'Sentiment': row['sentiment'],
        'Classification': classification,
        'Display Classification': display_classification,
        'Result': result,
        'Is Correct': is_correct
    })

    print(f"\nAvalia√ß√£o: {review_text}")
    print(f"Classifica√ß√£o do Gemini: {display_classification}")
    print("-" * 90)
    print(f"Sentimento Real: {row['sentiment'].capitalize()}")
    print(f"Resultado: {result}")

"""#### Comparativo de M√©tricas e Resultdos

##### An√°lise Consolidada
"""

df_openai = pd.DataFrame(openai_results)
df_gemini = pd.DataFrame(gemini_results)

accuracy_openai = df_openai['Is Correct'].mean() * 100
accuracy_gemini = df_gemini['Is Correct'].mean() * 100

correct_openai = df_openai['Is Correct'].sum()
wrong_openai = len(df_openai) - correct_openai

correct_gemini = df_gemini['Is Correct'].sum()
wrong_gemini = len(df_gemini) - correct_gemini

accuracy_summary = pd.DataFrame({
    'LLM': ['OpenAI GPT-4', 'Google Gemini 1.5'],
    'Accuracy (%)': [accuracy_openai, accuracy_gemini],
    'Correct Predictions': [correct_openai, correct_gemini],
    'Incorrect Predictions': [wrong_openai, wrong_gemini]
})

print("\nOverall Accuracy Summary:")
print(accuracy_summary)

"""##### G≈ïafico de Compara√ß√£o"""

plt.figure(figsize=(8, 6))

colors = ['#3498db', '#e74c3c']

bars = plt.bar(
    accuracy_summary['LLM'],
    accuracy_summary['Accuracy (%)'],
    color=colors
)

for bar in bars:
    height = bar.get_height()
    plt.text(
        bar.get_x() + bar.get_width() / 2,
        height + 1,
        f'{height:.1f}%',
        ha='center',
        va='bottom',
        fontsize=12,
        color='black'
    )

# Set y-limit and labels
plt.ylim(0, 100)
plt.ylabel('Accuracy (%)')
plt.title('LLM Sentiment Classification Accuracy Comparison')
plt.show()

"""##### Resultados Por Categoria"""

categories = ['positivo', 'negativo']

def get_category_performance(df, model_name, categories):
    """
    Returns a DataFrame with columns:
      - Model
      - Category
      - Total
      - Correct
      - Incorrect
      - Accuracy (%)
    for each category in `categories`.
    """
    actuals = df['Sentiment'].str.lower()
    preds = df['Classification'].str.lower()
    data = []

    for cat in categories:
        total = (actuals == cat).sum()
        correct = ((actuals == cat) & (preds == cat)).sum()
        incorrect = total - correct
        accuracy = (correct / total * 100) if total > 0 else 0

        data.append({
            'Model': model_name,
            'Category': cat,
            'Total': total,
            'Correct': correct,
            'Incorrect': incorrect,
            'Accuracy (%)': accuracy
        })

    return pd.DataFrame(data)

df_openai_cat = get_category_performance(df_openai, "OpenAI GPT-4", categories)
df_gemini_cat = get_category_performance(df_gemini, "Google Gemini 1.5", categories)

df_cat = pd.concat([df_openai_cat, df_gemini_cat], ignore_index=True)

print("\nPer-Category Performance Data:")
print(df_cat)

"""##### Gr√°fico dos Resultados Por Categoria"""

pivot_df = df_cat.pivot(index='Category', columns='Model', values='Accuracy (%)')

plt.figure(figsize=(8, 6))
pivot_df.plot(kind='bar', figsize=(8, 6))

plt.ylim(0, 100)
plt.ylabel('Accuracy (%)')
plt.title('Category-wise Accuracy Comparison')
plt.xticks(rotation=0)
plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

"""##### Report com Compara√ß√£o Detalhada"""

actuals_openai = df_openai['Sentiment'].str.lower()
preds_openai = df_openai['Classification'].str.lower()

actuals_gemini = df_gemini['Sentiment'].str.lower()
preds_gemini = df_gemini['Classification'].str.lower()

print("\nClassification Report for OpenAI GPT-4:")
print(classification_report(actuals_openai, preds_openai, labels=categories, zero_division=0))

print("Classification Report for Google Gemini 1.5:")
print(classification_report(actuals_gemini, preds_gemini, labels=categories, zero_division=0))

"""##### Matriz de Confus√£o"""

# Create confusion matrices for both models
cm_openai = confusion_matrix(actuals_openai, preds_openai, labels=categories)
cm_gemini = confusion_matrix(actuals_gemini, preds_gemini, labels=categories)

# Create a figure with 1 row and 2 columns, specifying horizontal space
fig, axs = plt.subplots(1, 2, figsize=(12, 5), gridspec_kw={'wspace': 0.5})

# Plot OpenAI GPT-4 confusion matrix on the first subplot
disp_openai = ConfusionMatrixDisplay(confusion_matrix=cm_openai, display_labels=categories)
disp_openai.plot(ax=axs[0], cmap='Blues', values_format='d')
axs[0].set_title('OpenAI GPT-4 Confusion Matrix')

# Plot Google Gemini 1.5 confusion matrix on the second subplot
disp_gemini = ConfusionMatrixDisplay(confusion_matrix=cm_gemini, display_labels=categories)
disp_gemini.plot(ax=axs[1], cmap='Blues', values_format='d')
axs[1].set_title('Google Gemini 1.5 Confusion Matrix')

plt.show()

"""# Conclus√£o

O projeto **NLP-Sentinel** mostrou que √© poss√≠vel alcan√ßar **alta precis√£o** na an√°lise de sentimentos em portugu√™s utilizando tanto modelos supervisionados quanto abordagens modernas com LLMs. A experimenta√ß√£o ampla com diferentes estrat√©gias ‚Äî desde embeddings e SVM at√© Transformers e modelos generativos ‚Äî permitiu identificar o equil√≠brio ideal entre **desempenho, custo computacional e simplicidade**.

Al√©m disso, o projeto revelou que abordagens tradicionais como **SVM com Bag of Words/Embeddings** e redes neurais **LSTM/RNN** n√£o se mostraram competitivas neste cen√°rio, sendo descontinuadas devido √† sua **baixa efici√™ncia e alto custo**.

A combina√ß√£o de pr√°ticas s√≥lidas de pr√©-processamento, curadoria de dados e testes controlados contribuiu para resultados robustos e replic√°veis. A utiliza√ß√£o do **Hugging Face Hub** para versionamento e compartilhamento de modelos tamb√©m facilitou a organiza√ß√£o do projeto.

Este trabalho refor√ßa que, para tarefas de PLN em ambientes reais, a escolha da abordagem ideal depende n√£o apenas de m√©tricas de acur√°cia, mas tamb√©m da **viabilidade de aplica√ß√£o no mundo real** ‚Äî considerando fatores como escalabilidade, recursos dispon√≠veis e tempo de resposta esperado.

Em resumo, o **NLP-Sentinel** alcan√ßou seu objetivo principal e ofereceu aprendizados valiosos sobre o uso de IA aplicada √† linguagem natural no contexto do portugu√™s brasileiro.
"""